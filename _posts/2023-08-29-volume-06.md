# Continuation of my journey with PyTorch

I have modified my project objective slightly differently now: 

- Create a portal that would allow my family to upload a photo
- After the photo is uploaded - the backend ML model would infer who is present in the photo and provide an output in the portal

For achieving this I would need to do the following: 

1. Create a ML model (preferably reuse existing model and retrain it) for identifying my family members
2. A website - I have learnt Streamlit for doing this work

The piece missing is a process to train a model that would work in all pieces of equipment ( laptop/ google colab/ container). I have got a lot of code that works only in my local system or only in Google CoLab; but not in both. I need to fine tune my PyTorch skills so that I need not import unnecessary libraries. I would continue to focus only on PyTorch Vision rather than Opencv. 

I am also planning to leverage PyTorch Lightning or PyTorch Ignite for handling boiler plate requirements. This would save me a lot of time. One of the key obstacles am facing is; all the examples am getting in books or the internet are making using of datasets built into the packages or from kaggle. I need to find the right way to transfer these techniques to work in my local CUDA. Also I need to avoid using unwanted minor libraries which is used by limited number of people. 
